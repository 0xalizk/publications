\section{Discussion}
Our simulation results show that computational intractability provides sufficient conditions for the emergence of the majority-leaves minority-hubs (mLmH) topology, irrespective of whether it follows a power-law distribution in a technical sense as has previously been intensely debated (see \cite{lima-mendez_powerful_2009} and references therein). The fact that the intractability of NP-hard problems is (assuming P$\neq$NP) universally insurmountable renders it a necessary condition as well: it rules out the possibility of any other topology. A completely sparse network where each gene has only one interaction produces the easiest possible optimization instances (every gene can unambiguously be either beneficial or detrimental under any evolutionary pressure scenario). However, it also leads to a need for more genes since functions that could have been handled by one hub gene of degree $d$ must now be handled by $d$ specialty genes. This obviously leads to an explosion of genome size. Conversely, a highly dense network where the number of genes is minimized and interactions are handled by multi-purpose hub genes leads to an exponential search space.  The number of iterations of random-variations and non-random selection \cite{carvunis_proto-genes_2012} before the network has been optimally rewired into a healthier state (i.e. the right subset genes has been conserved, discovered, mutated or deleted to overcome a given evolutionary pressure)  would be exponential in network size. The majority-leaves minority-hubs topology is the middle ground between these two extremes: concentrate essential functions in hubs genes \cite{gerstein_architecture_2012}, and respond to evolutionary pressure by experimenting (on the cheap) with loosely connected leaf genes at the periphery of the network \cite{kim_positive_2007}. Highly connected genes tend to perform essential functions \cite{khurana_interpretation_2013} and are unlikely to be detrimental in and of themselves. Regulating around them however (e.g. microRNA regulation \cite{gerstein_architecture_2012}) is where the constant optimization is needed. 
%NP-hardness is universally insurmountable, but what guarantee is there that the network evolution problem, as we defined it, captures the optimization dilemma of evolving systems? 
For the presented model to be applicable beyond explaining the emergence of mLmH, some limitations must be addressed. 
%The model presented here is not without limitations. 
We have treated all interactions (edges) as equal, but in reality some interactions are more potent than others. Unfortunately there is no large-scale data as of yet that can inform meaningful assignment of edge weights (i.e. some $\pm \alpha\in \mathbb{R}$ instead of simply $\pm$1). Alternatively potency can be estimated based on the centrality  of a given interaction (how many network shortest paths include it).  We have also treated all genes (nodes) as equal, but in reality a gene's position matters \cite{kim_positive_2007,gerstein_architecture_2012} and should be taken into account when attributing the magnitude of its benefit/damage scores. The benefit(damage) score of a central gene (many shortest paths pass through it) clearly has more positive (negative) impact on the network as a whole. Future work will extend the model beyond these limitations and apply its simulations as stress tests on experimentally validated regulatory networks (transcription-factor-gene, small-RNA-gene, gene-gene interactions) the coverage and accuracy of which is exponentially increasing \cite{han_trrust:_2015, neph_circuitry_2012, gerstein_architecture_2012, stergachis_conservation_2014}. In this regard, we intend to ask the following question: what subset pathways are involved in the hardest optimization instances under simulated evolutionary pressure? If the quick sands of computational intractability is the obstacle against Nature discovering a cancer-resistant regulatory network for example, the model may give a hint as to what subset of genes should be combinatorially optimized over (up- and down-regulation of genes). This can inform knockdown/out/in and RNA interference experiments, and is in sharp contrast with the dominant correlation-based cancer-target inference methods (which, even if statistically sound, do not necessarily reveal underlying causations). The efficacy of the model in making such predictions can easily be falsified against previously known cancer-implicated sets of genes. 
% from the ENCODE paper "Architecture of the human regulatory network derived from ENCODE data": 
% Highly connected network elements (both transcription factors and targets) are under strong evolutionary 
% selection and exhibit stronger allele-specific activity (this is particularly apparent when multiple factors are involved). 
%Surprisingly, however, elements with allelic activity are under weaker selection than non-allelic ones.
%  *** see also the child notes for this paper in Zotero